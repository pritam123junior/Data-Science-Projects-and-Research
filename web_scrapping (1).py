# -*- coding: utf-8 -*-
"""web_scrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Q1Lky0HRCqoajp3nDgcD8b7nz0ll49V
"""

pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup
import csv

# URL to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Send a request to the webpage
response = requests.get(url)

# Parse the webpage content
soup = BeautifulSoup(response.content, 'html.parser')

# Prepare the CSV file
with open('products.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)

    # Write the header
    writer.writerow(['Category', 'Product Name', 'Image URL', 'Price', 'Description'])

    # Find all products
    products = soup.find_all('div', class_='item')

    for product in products:
        # Extract the product name
        name = product.find('a', class_='product-title').text.strip()

        # Extract the image URL
        image_url = product.find('img', class_='product-image')['src']

        # Extract the price
        price = product.find('span', class_='price').text.strip()

        # Extract the description
        description = product.find('div', class_='description').text.strip()

        # Assuming the category is 'Women's Bags' based on the URL
        category = "Women's Bags"

        # Write the details to the CSV file
        writer.writerow([category, name, image_url, price, description])

print("Data has been scraped and saved to 'products.csv'")

pip install selenium beautifulsoup4

from selenium import webdriver
from bs4 import BeautifulSoup
import csv
import time

# Set up the WebDriver (replace with the path to your WebDriver)
driver = webdriver.Chrome(executable_path='C:\Users\RGON\Downloads\chromedriver-win64\chromedriver-win64')

# URL to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Open the webpage
driver.get(url)

# Wait for the page to load content (adjust time if needed)
time.sleep(5)

# Parse the webpage content using BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Prepare the CSV file
with open('products.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)

    # Write the header
    writer.writerow(['Category', 'Product Name', 'Image URL', 'Price', 'Description'])

    # Find all products
    products = soup.find_all('div', class_='a0c4')

    for product in products:
        # Extract the product name
        name = product.find('a', class_='tile-hover-target').text.strip()

        # Extract the image URL
        image_tag = product.find('img', class_='b0v8')
        image_url = image_tag['src'] if image_tag else 'No Image'

        # Extract the price
        price_tag = product.find('span', class_='ui-p5')
        price = price_tag.text.strip() if price_tag else 'No Price'

        # Extract the description (if available)
        description_tag = product.find('span', class_='d1w8')
        description = description_tag.text.strip() if description_tag else 'No Description'

        # Assuming the category is 'Women's Bags' based on the URL
        category = "Women's Bags"

        # Write the details to the CSV file
        writer.writerow([category, name, image_url, price, description])

print("Data has been scraped and saved to 'products.csv'")

# Close the WebDriver
driver.quit()

pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import csv
import time

# Path to your WebDriver (using raw string)
chromedriver_path = r'/content/drive/MyDrive/chromedriver-win64/chromedriver-win64/chromedriver.exe'


# Set up the WebDriver service
service = Service(chromedriver_path)

# Initialize the WebDriver with the service
driver = webdriver.Chrome(service=service)

# URL to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Open the webpage
driver.get(url)

# Wait for the page to load content (adjust time if needed)
time.sleep(5)

# Parse the webpage content using BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Prepare the CSV file
with open('products.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)

    # Write the header
    writer.writerow(['Category', 'Product Name', 'Image URL', 'Price', 'Description'])

    # Find all products
    products = soup.find_all('div', class_='a0c4')

    for product in products:
        # Extract the product name
        name = product.find('a', class_='tile-hover-target').text.strip()

        # Extract the image URL
        image_tag = product.find('img', class_='b0v8')
        image_url = image_tag['src'] if image_tag else 'No Image'

        # Extract the price
        price_tag = product.find('span', class_='ui-p5')
        price = price_tag.text.strip() if price_tag else 'No Price'

        # Extract the description (if available)
        description_tag = product.find('span', class_='d1w8')
        description = description_tag.text.strip() if description_tag else 'No Description'

        # Assuming the category is 'Women's Bags' based on the URL
        category = "Women's Bags"

        # Write the details to the CSV file
        writer.writerow([category, name, image_url, price, description])

print("Data has been scraped and saved to 'products.csv'")

# Close the WebDriver
driver.quit()

chmod +x /path/to/chromedriver



import os

chromedriver_path = r'/content/drive/MyDrive/chromedriver-win64/chromedriver-win64/chromedriver.exe'
if os.path.isfile(chromedriver_path):
    print("ChromeDriver found!")
else:
    print("ChromeDriver not found!")

from google.colab import drive
drive.mount('/content/drive')

pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Send a GET request to the website
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Find the category
category = soup.find('h1', class_='b6b7').text.strip()
print(f'Category: {category}')

# Find all products
products = soup.find_all('div', class_='b6b7')

for product in products:
    # Extract product name
    product_name = product.find('a', class_='b6b7').text.strip()

    # Extract product image URL
    product_image = product.find('img', class_='b6b7')['src']

    # Extract product price
    product_price = product.find('span', class_='b6b7').text.strip()

    # Extract product description
    product_description = product.find('p', class_='b6b7').text.strip()

    print(f'Product Name: {product_name}')
    print(f'Product Image: {product_image}')
    print(f'Product Price: {product_price}')
    print(f'Product Description: {product_description}')
    print('---')

import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Send a GET request to the website
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Find the category
category_tag = soup.find('h1', class_='b6b7')
if category_tag:
    category = category_tag.text.strip()
    print(f'Category: {category}')
else:
    print('Category not found')

# Find all products
products = soup.find_all('div', class_='b6b7')

for product in products:
    # Extract product name
    product_name_tag = product.find('a', class_='b6b7')
    product_name = product_name_tag.text.strip() if product_name_tag else 'N/A'

    # Extract product image URL
    product_image_tag = product.find('img', class_='b6b7')
    product_image = product_image_tag['src'] if product_image_tag else 'N/A'

    # Extract product price
    product_price_tag = product.find('span', class_='b6b7')
    product_price = product_price_tag.text.strip() if product_price_tag else 'N/A'

    # Extract product description
    product_description_tag = product.find('p', class_='b6b7')
    product_description = product_description_tag.text.strip() if product_description_tag else 'N/A'

    print(f'Product Name: {product_name}')
    print(f'Product Image: {product_image}')
    print(f'Product Price: {product_price}')
    print(f'Product Description: {product_description}')
    print('---')

import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = 'https://www.ozon.ru/category/sumki-zhenskie-17001/'

# Send a GET request to the website
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')

# Find the category
category_tag = soup.find('h1')
if category_tag:
    category = category_tag.text.strip()
    print(f'Category: {category}')
else:
    print('Category not found')

# Find all products
products = soup.find_all('div', class_='a0c4')

for product in products:
    # Extract product name
    product_name_tag = product.find('a', class_='a0c4')
    product_name = product_name_tag.text.strip() if product_name_tag else 'N/A'

    # Extract product image URL
    product_image_tag = product.find('img', class_='a0c4')
    product_image = product_image_tag['src'] if product_image_tag else 'N/A'

    # Extract product price
    product_price_tag = product.find('span', class_='a0c4')
    product_price = product_price_tag.text.strip() if product_price_tag else 'N/A'

    # Extract product description
    product_description_tag = product.find('p', class_='a0c4')
    product_description = product_description_tag.text.strip() if product_description_tag else 'N/A'

    print(f'Product Name: {product_name}')
    print(f'Product Image: {product_image}')
    print(f'Product Price: {product_price}')
    print(f'Product Description: {product_description}')
    print('---')

import requests
from bs4 import BeautifulSoup
import csv

# Define the URL of the website you want to scrape
url = "https://www.daraz.com.bd/?spm=a2a0e.tm80335401.header.dhome#hp-categories"

# Send a GET request to the website
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.content, 'html.parser')

    # Open a CSV file to write the data
    with open('daraz_products.csv', mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['Category', 'Product', 'Image', 'Price', 'Description'])

        # Extract the categories, products, images, prices, and descriptions
        categories = soup.find_all('div', class_='category-class')  # Adjust the class name
        for category in categories:
            category_name = category.text.strip()
            products = category.find_all('div', class_='product-class')  # Adjust the class name

            for product in products:
                product_name = product.find('a', class_='product-name-class').text.strip()  # Adjust the class name
                image = product.find('img', class_='image-class')['src']  # Adjust the class name
                price = product.find('span', class_='price-class').text.strip()  # Adjust the class name
                description = product.find('p', class_='description-class').text.strip()  # Adjust the class name

                # Write the data to the CSV file
                writer.writerow([category_name, product_name, image, price, description])

    print("Data has been successfully scraped and saved to daraz_products.csv")
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

url = 'https://www.ozon.ru/category/aksessuary-7697/'

# Create a session
session = requests.Session()

# Set headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

# Send a GET request to the website
response = session.get(url, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    print("Page retrieved successfully.")
    soup = BeautifulSoup(response.text, 'html.parser')

    # Step 5: Scrape the Data
    products = soup.find_all('div', class_='product-card')  # Adjust the class name as needed

    for product in products:
        try:
            # Extract  prodct details
            product_name = product.find('h2', class_='product-title').text.strip()  # Adjust selector
            product_price = product.find('span', class_='product-price').text.strip()  # Adjust selector
            print(f'Product Name: {product_name}, Price: {product_price}')
        except AttributeError as e:
            print("Error extracting product details:", e)

else:
    print(f"Failed to retrieve page. Status code: {response.status_code}")
    print("Response text:", response.text)  # Print the response text for debugging